{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG-LFVUOthAp"
      },
      "source": [
        "# **Important Installations & Dependencies**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KaosE05Cbju"
      },
      "source": [
        "from keras.preprocessing import sequence, text\n",
        "from keras.models import Model\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from sklearn import preprocessing\n",
        "from time import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import csv\n",
        "from sklearn.metrics import (\n",
        "    classification_report as creport\n",
        ")\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score, precision_score,recall_score\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2cBUWdu9N63-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8qI_9rKwDoT"
      },
      "source": [
        "# Arabic Word Embeddings (AraVec Or fasttext) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGfF1geNSzZ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "b896080b-9bc8-4cc3-ef13-342669ab8945"
      },
      "source": [
        "! unzip '/content/drive/My Drive/New- test/tweets_sg_300.zip'  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/New- test/tweets_sg_300.zip\n",
            "  inflating: tweets_sg_300           \n",
            "  inflating: tweets_sg_300.trainables.syn1neg.npy  \n",
            "  inflating: tweets_sg_300.wv.vectors.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ar.300.bin.gz\n",
        "!gunzip cc.ar.300.bin.gz"
      ],
      "metadata": {
        "id": "OPR8KAY_Vu4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJLejALUS3w2"
      },
      "source": [
        "# Word_embedding_path\n",
        "embedding_path = '/content/tweets_sg_300'           # change the path to '/content/cc.ar.300.bin' when fasttext is needed to use"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDCeUyipOdAI"
      },
      "source": [
        "X = []\n",
        "y = []\n",
        "for data_path in [\"/content/drive/MyDrive/DatasetCleaned/data_train.csv\"]:\n",
        "     with open(data_path, 'r') as f:\n",
        "          for i, line in enumerate(f):\n",
        "              if i == 0: continue\n",
        "              else:\n",
        "                  temp = line.split(',')\n",
        "                  X.append(temp[0].split())\n",
        "                  y.append(temp[1].replace('\\n', ''))\n",
        "X, y = np.array(X), np.array(y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDQuidU8KhpN"
      },
      "source": [
        "X_test = []\n",
        "y_test = []\n",
        "for data_path in [\"/content/drive/MyDrive/DatasetCleaned/data_test.csv\"]:\n",
        "     with open(data_path, 'r') as f:\n",
        "          for i, line in enumerate(f):\n",
        "              if i == 0: continue\n",
        "              else:\n",
        "                  temp = line.split(',')\n",
        "                  X_test.append(temp[0].split())\n",
        "                  y_test.append(temp[1].replace('\\n', ''))\n",
        "X_test, y_test = np.array(X_test), np.array(y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUeVuH2B0akw"
      },
      "source": [
        "**## Get Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxim2tYsGQ3b"
      },
      "source": [
        "def get_init_parameters(path, ext=None):\n",
        "    word_model = KeyedVectors.load(path).wv\n",
        "    n_words = len(word_model.vocab)\n",
        "    vocab_dim = word_model[word_model.index2word[0]].shape[0]\n",
        "    index_dict = dict()\n",
        "    for i in range(n_words):\n",
        "        index_dict[word_model.index2word[i]] = i+1\n",
        "    print('Number of words in the word embedding',n_words)\n",
        "    #print('word_model', word_model)\n",
        "    #print(\"index_dict\",index_dict)\n",
        "    return word_model, index_dict, n_words, vocab_dim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll909TRTG0Yd"
      },
      "source": [
        "WORD_MODEL, index_dict, MAX_FEATURES, EMBED_SIZE = get_init_parameters(embedding_path) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xaUdzMs0O_-"
      },
      "source": [
        "EMBED_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt-j40bMG6U3"
      },
      "source": [
        "len(index_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZKwTnzYJUxX"
      },
      "source": [
        "def get_word_index(train_raw_text, test_raw_text, n_words):\n",
        "    tokenizer = text.Tokenizer(num_words=n_words)\n",
        "    tokenizer.fit_on_texts(list(train_raw_text))\n",
        "    word_index = tokenizer.word_index\n",
        "\n",
        "    return word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aPXVJqTJhjY"
      },
      "source": [
        "word_index  = get_word_index(X,X_test,MAX_FEATURES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzJ2fELYG8wc"
      },
      "source": [
        "def w2v(word_index, embedding_index, vocab_dim):\n",
        "    print('Building embedding matrix...')\n",
        "    dicc={}\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, vocab_dim))\n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            embedding_matrix[i] = embedding_index.get_vector(word)\n",
        "        except:\n",
        "            pass\n",
        "        dicc[word]= embedding_matrix[i]\n",
        "\n",
        "    print('Embedding matrix built.') \n",
        "    #print(\"Word index\", word_index.items())\n",
        "    #print(embedding_matrix) \n",
        "    return dicc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPuD1ZC-IRa_"
      },
      "source": [
        "dicc= w2v(word_index, WORD_MODEL, EMBED_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgMBeUP2IXqi"
      },
      "source": [
        "len(dicc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOZzXhcrL1X_"
      },
      "source": [
        "\"\"\"\n",
        "To use the word embeddings with the classical machine learning models, \n",
        "the average vector of all the embeddings of the tweet words is computed\n",
        "\"\"\"\n",
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, dicc):\n",
        "        self.dicc = dicc\n",
        "        if len(dicc)>0:\n",
        "            self.dim=300\n",
        "        else:\n",
        "            self.dim=0\n",
        "            \n",
        "    def fit(self, X, y):\n",
        "        return self \n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.mean([self.dicc[w] for w in words if w in self.dicc] \n",
        "                    or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brnQ7H9HziTC"
      },
      "source": [
        "# **Training & Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7C0u4-a1XbW"
      },
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "NB_w2v = Pipeline([\n",
        "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(dicc)),\n",
        "    (\"NB_w2v\",  naive_bayes.BernoulliNB())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4korvF21XbW"
      },
      "source": [
        "NB_w2v= NB_w2v.fit(X, y)\n",
        "predictions_NB= NB_w2v.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIcbQxvE1XbW"
      },
      "source": [
        "print(\"NB weighted-averaged precision-score -> \",precision_score(y_test, predictions_NB,average=\"macro\", pos_label=\"1\"))\n",
        "print(\"NB weighted-averaged recall-score -> \",recall_score(y_test, predictions_NB,average=\"macro\", pos_label=\"1\"))\n",
        "print(\"NB weighted-averaged F1-score -> \",f1_score(y_test, predictions_NB,average=\"macro\", pos_label=\"1\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kuinskS1XbX"
      },
      "source": [
        "print(creport(y_test, predictions_NB,target_names=['Offensive', 'Benign'],digits=4))\n",
        "# print(creport(y_test, predictions_NB,target_names=['Literature,Sports, Judiciary, Politics, Art, Business'],digits=4)) // For Multi dataset"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
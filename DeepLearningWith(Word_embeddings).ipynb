{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3psLV0AyPwSV"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnSw0od87C8-"
      },
      "source": [
        "from keras.layers import Embedding, Dense, Dropout, Input, LSTM, Bidirectional\n",
        "from keras.layers import MaxPooling1D, Conv1D, Flatten, TimeDistributed\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.models import Model\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import Callback\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from sklearn import preprocessing\n",
        "from time import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import csv\n",
        "from keras import optimizers\n",
        "from sklearn.metrics import f1_score, precision_score,recall_score\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import (\n",
        "    classification_report as creport\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6_dt2cDtobO"
      },
      "source": [
        "# Arabic Word Embeddings (AraVec Or fasttext) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mn0KDJg-o5u"
      },
      "source": [
        "#AraVec Source: https://github.com/bakrianoo/aravec/tree/master/AraVec%202.0\n",
        "! unzip '/content/drive/My Drive/New- test/tweets_sg_300.zip'  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ar.300.bin.gz\n",
        "!gunzip cc.ar.300.bin.gz"
      ],
      "metadata": {
        "id": "OPR8KAY_Vu4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJLejALUS3w2"
      },
      "source": [
        "# Word_embedding_path\n",
        "embedding_path = '/content/tweets_sg_300'           # change the path to '/content/cc.ar.300.bin' when fasttext is needed to use"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae5n6EhUcEDD"
      },
      "source": [
        "def get_embedding_matrix(word_index, embedding_index, vocab_dim):\n",
        "    print('Building embedding matrix...')\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, vocab_dim))\n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            embedding_matrix[i] = embedding_index.get_vector(word)\n",
        "        except:\n",
        "            pass\n",
        "    print('Embedding matrix built.') \n",
        "    #print(\"Word index\", word_index.items())\n",
        "    #print(embedding_matrix) \n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def get_init_parameters(path, ext=None):\n",
        "    if ext == 'vec':\n",
        "        word_model = KeyedVectors.load_word2vec_format(path).wv\n",
        "    else:\n",
        "        word_model = KeyedVectors.load(path).wv\n",
        "    n_words = len(word_model.vocab)\n",
        "    vocab_dim = word_model[word_model.index2word[0]].shape[0]\n",
        "    index_dict = dict()\n",
        "    for i in range(n_words):\n",
        "        index_dict[word_model.index2word[i]] = i+1\n",
        "    print('Number of words in the word embedding',n_words)\n",
        "    #print('word_model', word_model)\n",
        "    #print(\"index_dict\",index_dict)\n",
        "    return word_model, index_dict, n_words, vocab_dim\n",
        "\n",
        "def get_max_length(text_data, return_line=False):\n",
        "    max_length = 0\n",
        "    long_line = \"\"\n",
        "    for line in text_data:\n",
        "        new = len(line.split())\n",
        "        if new > max_length:\n",
        "            max_length = new\n",
        "            long_line = line\n",
        "    if return_line:\n",
        "        return long_line, max_length\n",
        "    else:\n",
        "        return max_length\n",
        "    print(\"max\",long_line,max_length)\n",
        "\n",
        "def load_datasets(data_paths, header=True):\n",
        "    x = []\n",
        "    y = []\n",
        "    for data_path in data_paths:\n",
        "        with open(data_path, 'r') as f:\n",
        "            for line in f:\n",
        "                if header:\n",
        "                    header = False\n",
        "                else:\n",
        "                    temp = line.split(',')\n",
        "                    x.append(temp[0])\n",
        "                    y.append(temp[2].replace('\\n', ''))\n",
        "    max_length = get_max_length(x)\n",
        "    print('Max length:', max_length)\n",
        "    return x,y, max_length\n",
        "\n",
        "def get_train_test(train_raw_text, test_raw_text, n_words, max_length):\n",
        "    tokenizer = text.Tokenizer(num_words=n_words)\n",
        "    tokenizer.fit_on_texts(list(train_raw_text))\n",
        "    word_index = tokenizer.word_index\n",
        "   \n",
        "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
        "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
        "\n",
        "    return sequence.pad_sequences(train_tokenized, maxlen=max_length, padding='post', truncating='post'),\\\n",
        "           sequence.pad_sequences(test_tokenized, maxlen=max_length, padding='post', truncating='post'),\\\n",
        "           word_index\n",
        "\n",
        "def class_str_2_ind(x_train, x_test, y_train, y_test, classes, n_words, max_length):\n",
        "    print('Converting data to trainable form...')\n",
        "    y_encoder = preprocessing.LabelEncoder()\n",
        "    y_encoder.fit(classes)\n",
        "    y_train = y_encoder.transform(y_train)\n",
        "    y_test = y_encoder.transform(y_test)\n",
        "    #print(y_train)\n",
        "    #print(y_test)\n",
        "    train_y_cat = np_utils.to_categorical(y_train, len(classes))\n",
        "    x_vec_train, x_vec_test, word_index = get_train_test(x_train, x_test, n_words, max_length)\n",
        "    print('Number of training examples: ' + str(len(x_vec_train)))\n",
        "    print('Number of testing examples: ' + str(len(x_vec_test)))\n",
        "    return x_vec_train, x_vec_test, y_train, y_test, train_y_cat, word_index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT0r5MyUbtGC",
        "outputId": "629270b0-e874-46e0-acef-b4d419458efa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "WORD_MODEL, _, MAX_FEATURES, EMBED_SIZE = get_init_parameters(embedding_path) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of words in the word embedding 331679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjwJ3aN3U1Mv"
      },
      "source": [
        "# load training data\n",
        "train_data_path=[\"/content/drive/MyDrive/DatasetCleaned/data_train.csv\"]\n",
        "x_train, y_train, MAX_TEXT_LENGTH = load_datasets(train_data_path)\n",
        "CLASSES_LIST = np.unique(y_train)\n",
        "print('Label categories: ' + str(CLASSES_LIST))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGShWPV6UwCE"
      },
      "source": [
        "# load testing data\n",
        "test_data_path=[\"/content/drive/MyDrive/DatasetCleaned/data_test.csv\"]\n",
        "x_test, y_test, MAX_TEXT_LENGTH = load_datasets(test_data_path)\n",
        "CLASSES_LIST = np.unique(y_test)\n",
        "print('Label categories: ' + str(CLASSES_LIST))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plOnSpgUb18i"
      },
      "source": [
        "x_train, x_test, y_train, y_test, train_y_cat, word_index = class_str_2_ind(x_train, x_test, \n",
        "                                                                            y_train, y_test,\n",
        "                                                                            CLASSES_LIST, MAX_FEATURES,\n",
        "                                                                            MAX_TEXT_LENGTH)\n",
        "test_cat_y = np_utils.to_categorical(y_test, len(CLASSES_LIST))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQiq-BKShbLE"
      },
      "source": [
        "print(\"Tokens number: \"+str(len(word_index)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjJd-CWUiNlQ"
      },
      "source": [
        "# Sequence length\n",
        "print(\"Original sequence length: \"+str(MAX_TEXT_LENGTH))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRIAZtDUidul"
      },
      "source": [
        "def get_model(embedding_weights, word_index, vocab_dim, max_length,layer, dropout, optimizer, print_summary=True):\n",
        "    \"\"\"\n",
        "    Create Neural Network With an Embedding layer\n",
        "    \"\"\"\n",
        "    inp = Input(shape=(max_length,))\n",
        "    model = Embedding(input_dim=len(word_index)+1,\n",
        "                      output_dim=vocab_dim,\n",
        "                      trainable=False,\n",
        "                      weights=[embedding_weights])(inp)\n",
        "    model = layer(model)\n",
        "    model = Dropout(dropout)(model)       \n",
        "    model = Flatten()(model)\n",
        "    model = Dense(2, activation='sigmoid')(model)\n",
        "    model = Model(inputs=inp, outputs=model)\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    if print_summary:\n",
        "        model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH, \n",
        "                   layer, dropout,optimizer):\n",
        "   \n",
        "    tmp = get_embedding_matrix(word_index, WORD_MODEL, EMBED_SIZE)\n",
        "    model = get_model(tmp, word_index, EMBED_SIZE, MAX_TEXT_LENGTH, \n",
        "                      layer, dropout, optimizer= optimizer ,print_summary=True)\n",
        "    return model\n",
        "\n",
        "\n",
        "class TestCallback(Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        x, y = self.test_data\n",
        "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
        "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
        "\n",
        "def train_fit_predict(model, x_train, x_test, y_train, y_test,class_weight, batch_size, epochs, TestCallback=TestCallback):\n",
        "   \n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs, verbose=1,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        class_weight=class_weight,\n",
        "                        callbacks=[TestCallback((x_test, y_test))])\n",
        "    return history, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-XRhkE5wNkw"
      },
      "source": [
        "# RNN (BLSTM) Model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOhvzj0WXZ-g"
      },
      "source": [
        "model = get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH, \n",
        "                       layer= Bidirectional(LSTM(units=32, return_sequences=True, return_state=False)), \n",
        "                       dropout=0.2, optimizer=optimizers.Adam())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i62FHvSzXk7Q"
      },
      "source": [
        "time_start = time()\n",
        "history, model = train_fit_predict(model,\n",
        "                               x_train[:, :MAX_TEXT_LENGTH],\n",
        "                               x_test[:, :MAX_TEXT_LENGTH],\n",
        "                               train_y_cat, test_cat_y, class_weight=None,\n",
        "                               batch_size=500, epochs=15)\n",
        "time_start = time() - time_start\n",
        "\n",
        "print(\"Took : \"+str(np.round(time_start, 2))+\" (s)\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWTJMbAqX5Gj"
      },
      "source": [
        "model.evaluate(x_test[:, :MAX_TEXT_LENGTH], test_cat_y, batch_size=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf0CVyJrX-hu"
      },
      "source": [
        "y_pred = np.argmax(model.predict(x_test[:, :MAX_TEXT_LENGTH]), axis=1)\n",
        "\n",
        "print(creport(np.argmax(test_cat_y, axis=1), y_pred,target_names=['Offensive', 'Benign'],digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjkr6-woi004"
      },
      "source": [
        "plot_model(model, to_file='RNN_BLSTM_model.png', show_shapes=False, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model \n"
      ],
      "metadata": {
        "id": "BPZwHJfzgJGA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgYTtJZpg-rU"
      },
      "source": [
        "def get_model(embedding_weights, word_index, vocab_dim, max_length, print_summary=True):\n",
        "    \"\"\"\n",
        "    Create Neural Network With an Embedding layer\n",
        "    \"\"\"\n",
        "    inp = Input(shape=(max_length,))\n",
        "    model = Embedding(input_dim=len(word_index)+1,\n",
        "                      output_dim=vocab_dim,\n",
        "                      trainable=False,\n",
        "                      weights=[embedding_weights])(inp)\n",
        "\n",
        "    model = Conv1D(filters=25, kernel_size=5, padding='same', activation='relu')(model)\n",
        "    model = MaxPooling1D(pool_size=2)(model)\n",
        "    model = Flatten()(model)\n",
        "   \n",
        "    model = Dense(2, activation='sigmoid')(model)\n",
        "    model = Model(inputs=inp, outputs=model)\n",
        "    \n",
        "    from keras import optimizers\n",
        "\n",
        "    opt = optimizers.adam(lr=0.0001)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    if print_summary:\n",
        "        model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH):\n",
        "    tmp = get_embedding_matrix(word_index, WORD_MODEL, EMBED_SIZE)\n",
        "    model = get_model(tmp, word_index, EMBED_SIZE, MAX_TEXT_LENGTH, print_summary=True)\n",
        "    return model\n",
        "\n",
        "\n",
        "class TestCallback(Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        x, y = self.test_data\n",
        "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
        "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
        "\n",
        "def train_fit_predict(model, x_train, x_test, y_train, y_test, batch_size, epochs, TestCallback=TestCallback):\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs, verbose=1,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        callbacks=[TestCallback((x_test, y_test))])\n",
        "    return history, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKpEhF4ljGqR"
      },
      "source": [
        "model = get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIdYkbZzjJpe"
      },
      "source": [
        "time_start = time()\n",
        "history, model = train_fit_predict(model,\n",
        "                               x_train[:, :MAX_TEXT_LENGTH],\n",
        "                               x_test[:, :MAX_TEXT_LENGTH],\n",
        "                               train_y_cat, test_cat_y,\n",
        "                               batch_size=500, epochs=15)\n",
        "time_start = time() - time_start\n",
        "\n",
        "print(\"Took : \"+str(np.round(time_start, 2))+\" (s)\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt-l_Q9Aj44w"
      },
      "source": [
        "history.history.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xsxi8A1j_Fq"
      },
      "source": [
        "model.evaluate(x_test[:, :MAX_TEXT_LENGTH], test_cat_y, batch_size=1000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1e78k51kZ1S"
      },
      "source": [
        "y_pred = np.argmax(model.predict(x_test[:, :MAX_TEXT_LENGTH]), axis=1)\n",
        "\n",
        "print(creport(np.argmax(test_cat_y, axis=1), y_pred,target_names=['Offensive', 'Benign'],digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZp0EW06kwhy"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='CNN_model.png', show_shapes=False, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXB7MDh9lLcM"
      },
      "source": [
        "#  CNN+RNN(BLSTM) model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD0riiBniJn7"
      },
      "source": [
        "def get_model(embedding_weights, word_index, vocab_dim, max_length,layer, dropout, optimizer, print_summary=True):\n",
        "    \"\"\"\n",
        "    Create Neural Network With an Embedding layer\n",
        "    \"\"\"\n",
        "    inp = Input(shape=(max_length,))\n",
        "    model = Embedding(input_dim=len(word_index)+1,\n",
        "                      output_dim=vocab_dim,\n",
        "                      trainable=False,\n",
        "                      weights=[embedding_weights])(inp)\n",
        "\n",
        "    model = Conv1D(filters=25, kernel_size=5, padding='same', activation='relu', input_shape=(None, 84, 300) )(model)\n",
        "    model = MaxPooling1D(pool_size=2)(model)\n",
        "    model= TimeDistributed(Dense(32))(model)   \n",
        "                       \n",
        "    model = layer(model)\n",
        "    model = Dropout(dropout)(model)  \n",
        "         \n",
        "    model = Flatten()(model)\n",
        "    model = Dense(2, activation='sigmoid')(model)\n",
        "    model = Model(inputs=inp, outputs=model)\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    if print_summary:\n",
        "        model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH, \n",
        "                   layer, dropout,optimizer):\n",
        "   \n",
        "    tmp = get_embedding_matrix(word_index, WORD_MODEL, EMBED_SIZE)\n",
        "    model = get_model(tmp, word_index, EMBED_SIZE, MAX_TEXT_LENGTH, \n",
        "                      layer, dropout, optimizer= optimizer ,print_summary=True)\n",
        "    return model\n",
        "\n",
        "\n",
        "class TestCallback(Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        x, y = self.test_data\n",
        "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
        "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
        "\n",
        "def train_fit_predict(model, x_train, x_test, y_train, y_test,class_weight, batch_size, epochs, TestCallback=TestCallback):\n",
        "   \n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs, verbose=1,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        class_weight=class_weight,\n",
        "                        callbacks=[TestCallback((x_test, y_test))])\n",
        "    return history, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMeeL-Gqfy6p"
      },
      "source": [
        "model = get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH, \n",
        "                       layer= Bidirectional(LSTM(units=32, return_sequences=True, return_state=False)), \n",
        "                       dropout=0.2, optimizer= optimizers.Adam())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEQH3DdfYLdL"
      },
      "source": [
        "time_start = time()\n",
        "history, model = train_fit_predict(model,\n",
        "                               x_train[:, :MAX_TEXT_LENGTH],\n",
        "                               x_test[:, :MAX_TEXT_LENGTH],\n",
        "                               train_y_cat, test_cat_y, class_weight=None,\n",
        "                               batch_size=500, epochs=15)\n",
        "time_start = time() - time_start\n",
        "\n",
        "print(\"Took : \"+str(np.round(time_start, 2))+\" (s)\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE3iQ2bTYQjC"
      },
      "source": [
        "model.evaluate(x_test[:, :MAX_TEXT_LENGTH], test_cat_y, batch_size=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtSpZC1_YSiN"
      },
      "source": [
        "y_pred = np.argmax(model.predict(x_test[:, :MAX_TEXT_LENGTH]), axis=1)\n",
        "\n",
        "print(creport(np.argmax(test_cat_y, axis=1), y_pred,target_names=['Offensive', 'Benign'],digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js9z3MI-YWqR"
      },
      "source": [
        "plot_model(model, to_file='CNN_BLSTM_model.png', show_shapes=False, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}